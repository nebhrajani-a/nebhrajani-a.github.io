#+include: ../../preamble.org

#+HTML: <h1 id="title">New Age Software Development</h1>
#+HTML: <div class="post-meta"> <2020-10-22 Thu> - 4 min read
#+HTML: </div>

* UNIX Tools

Donald Knuth, demonstrating literate programming, once
wrote a ten-page program to count the \(k\) most
frequently used words in an input file, and print out a sorted list of the
words with their frequencies. Knuth's paper was reviewed
by McIlroy, who added at the end of his review a six
command pipeline (using UNIX tools), that also solves the
problem.

#+BEGIN_SRC sh
  tr -cs A-Za-z '\n' |
  tr A-Z a-z |
  sort |
  uniq -c |
  sort -rn |
  sed ${1}q
#+END_SRC
For those unfamiliar with the UNIX command line, this
pipeline takes the input text stream,
transliterates the complement (-c) of the alphabet set to newlines,
and squeezes (-s) multiple newlines. It then 'pipes' this
stream into the next command, which transliterates all
capital letters to lowercase ones. These are then sorted
to bring identical words together, and duplicates removed
using =uniq=, with a count added (-c). Again
sorted in reverse numeric order, since the first 'column'
of the stream is the frequency. The top \(k\) lines are
then printed using the stream editor (taken as the first
argument, ${1}). This could've been done
using =head= as well, but perhaps it wasn't
around at the time.


This example perfectly illustrates the power of the UNIX
command line and philosophy. Although Knuth's solution was
more efficient ($O(n\log{n})$ vs $O(\log{n})$ IIRC),
the beauty lies in the way UNIX works: it's a set of tiny
programs that don't do much alone, but together are able
to perform complex tasks one would have to write a
monolithic program to solve. This is the right way to
write software: since it does not leave the end user
trapped in the mind and imagination of the developer: the
user can do whatever he wants with a box of tools. This
has an analogue in mechanical work: would you choose one
tool that does a few tasks in a mediocre way, or carry a
toolbox that can do any number of tasks by using the tools
in combination? Yes: modular is better.

* Monolithic Software

            The software people use today is nothing like the UNIX
            shell
            (usually [[https://en.wikipedia.org/wiki/Bash_(Unix_shell)][bash]].
            It's usually a gigantic program that does the functions of
            many smaller programs. A popular example (from a brilliant
            essay
            called [[http://cristal.inria.fr/~weis/info/commandline.html][In The Beginning Was The Command Line]], is that of
            the program =wc= being replaced by a feature
            in, say, MS Word that counts all the words. In essence,
            modern programs accumulate features, combining the
            features of more and more smaller programs and integrating
            them. This probably started with the Kitchen Sink, Emacs,
            but was quickly adopted by many others. Let me be clear.
            This approach isn't necessarily wrong or less good: but it
            does mean one thing: you'll reach a point where
            two /huge/ programs just can't be combined with each
            other. Maybe it's because they were built by different
            people, or to solve different problems: but they can't
            communicate with each other using pipes and text streams
            anymore, for instance, Word and \(\mathrm{\LaTeX}\). This
            is where the problems begin:
            1. Big programs can't work together neatly. They use
              different file formats, they're doing different tasks,
              and a huge amount of code has to be written to make
              these programs talk to each other.
            2. De-facto standards are set, giving one piece of
               software a monopoly. By definition, such software will
               be the easiest to use even for a beginner. However,
               easy to use tools are seldom the best possible tools
               for the job. Consider Microsoft Word. Easy to use,
               requires no reading of the manual, but cannot handle
               serious writing well at all. Besides, such software
               generates monopolies: the 90's Microsoft monopoly
               wasn't very long ago.


            What does this eventually mean for software as a whole?
            It's a net loss for humanity. Instead of using tools of
            mathematical precision, we tend to use the only tool we
            know how. The law of the instrument plays out, eventually
            resulting in nobody but the intellectual 'elite' using the
            right tools for the job.


            I'm not sure how software gets away with this. Everywhere
            else (think back to car mechanics), eventually, people
            learn to use the right tool and adopt it quickly. However,
            it seems that this does not hold true for software. Is it
            really that hard to read a manual and to spend a few days
            learning to use, say, vim? It does pay off in the long
            run. Maybe it's human laziness: it's much easier to click
            around and guess at how to use software than reading its
            manual.

* Conclusion

Steve Jobs often touted how people completely unfamiliar
with computers could quickly use iOS without much help due
to its simple design. He was right: using iPhones is a lot
easier than using Linux, and requires far less training.
Maybe that's a good thing: but it over-panders to a class
of users having no prior experience with computers:
primarily children and grandparents. However, there is an
increasingly large number of people today using crappy
tools, despite having a fair amount of computer literacy:
the secretaries, the administrators, the government, the
police. That's a potential market to tap into.
